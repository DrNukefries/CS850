\documentclass{beamer}

\let\val\undefined
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{grffile}
\usepackage{graphicx}

\usetheme[progressbar=frametitle]{metropolis}
%\usepackage{libertine}

% *** Styles ***
%\setbeamertemplate{navigation symbols}{}
%\usecolortheme{dolphin}
%\usecolortheme{rose}
%\setbeamercovered{transparent}
%\usefonttheme{professionalfonts}
%\usefonttheme[onlymath]{serif}

% *** Colors ***
\newcommand{\tc}[2]{\textcolor{#1}{#2}}
\newcommand{\tcb}[1]{\tc{blue}{#1}}
\newcommand{\tcr}[1]{\tc{red}{#1}}
\newcommand{\tcg}[1]{\tc{green}{#1}}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\newcommand{\Ex}{\mathbb{E}}
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\sd}{\operatorname{sd}}
\newcommand{\cov}{\operatorname{Cov}}
\newcommand{\corr}{\operatorname{corr}}
\renewcommand{\P}[1]{\mathbb{P}\left[#1\right]}

\definecolor{varcolor}{RGB}{132,23,49}
\newcommand{\varname}[1]{\textcolor{varcolor}{\mathsf{#1}}}

\title{Probability 2}
\subtitle{MML 6.2, 6.3}
\author{Marek Petrik}
\date{8/30/2023}

\begin{document}
\begin{frame}
\maketitle
\end{frame}

\begin{frame} \frametitle{Probability Space}
\begin{itemize}
\item \textbf{Outcome space}: Set $\Omega$
  \vfill 
  % set of all possible outcomes
  % example: {J(ohn), E(ve), M(ary)}
\item \textbf{Event space} (sigma algebra): $\mathcal{F} \subset 2^{\Omega}$
  \vfill
  % all possible events, more than a single person can have a property
  % example {0, {J}, {E}, .., {J,E}, {J,M}, ..., {J,E,M}}
  % for finite \Omega, the set is typically a power set
\item \textbf{Probability function}: $P \colon \mathcal{F} \to [0,1]$
  % measures the probability of every student
  % could represent the probability of a student showing up for a class =
  % = relatively how often the student shows up for school
  % P(J) = 0.5, P(E) = 0.25, P(M) = 0.25
\end{itemize}
% \vfill \pause
% \textbf{Example}
% \begin{itemize}
% \item \textbf{Outcome space}: UNH students $\mathcal{U} = \{ \text{Jane}, \text{John}, \dots \}$ 
% \item \textbf{Even space}: All possible student groups: $2^{\mathcal{U}}$, must contain countable intersection, union, complement, and $\Omega$
% \item \textbf{Probability function}: $P(u), u \in \mathcal{U}$ probability of seeing a student $u$ on a random day on campus.
% \end{itemize}
% \vfill \pause
% \alert{What is the probability $P(\{u_1, u_2\})$ ($u_1 \neq u_2$)?}
% \pause
% \[ P(A \cup B) = P(A) + P(B) \text{ when } A \cap B = \emptyset  \]
\end{frame}

\begin{frame} \frametitle{Random Variable}
  A $\mathcal{T}$-valued random variable $X$ (upper case!) is a function:
  \[ X \colon \Omega \to \mathcal{T}\]
    Examples \( \Omega = \{ J, E, M \} \):
    \vspace{4cm}
\end{frame}

\begin{frame} \frametitle{Random Variable: Pre-image (inverse)}
  Pre-image \( X^{-1} \colon \mathcal{T} \to 2^{\Omega}  \) defined as
  \[
   X^{-1}(x) = \left\{ \omega \in \Omega \mid X(\omega) = x  \right\} 
  \]
  Examples \( \Omega = \{ J, E, M \} \):
  \vspace{4cm} 
\end{frame}


\begin{frame} \frametitle{Probability Distribution}
  Always associated with a random variable for some $X\colon \Omega \to \mathcal{T}$
  \[
    \P{X = x} = P\left(X^{-1}(x)\right) = P \left(  \left\{ \omega \in \Omega \mid X(\omega) = x  \right\} \right)
  \]
  \vspace{4cm}
\end{frame}

\begin{frame} \frametitle{Today}
  \begin{enumerate}
  \item Discrete probability distribution
  \item Joint distribution
  \item Sum rule, product rule
  \item Bayes theorem
  \item Continuous probability distribution
  \item Expectation(maybe)
  \end{enumerate}
\end{frame}

\begin{frame} \frametitle{Joint Probability}
  Discrete random variables $X$, $Y$
  \[
   \P{X = x, Y = y} \; =\;  P(X^{-1}(x) \cap Y^{-1}(y)) = \ldots
  \]
  Probability mass function: $p_{X,Y}(x,y)$ \\[4cm]
\end{frame}

\begin{frame} \frametitle{Marginalization (Sum Rule)}
  Know $\P{X = x, Y = y}$ and need to compute $\P{X = x}$:
  \[
   \P{X = x}\;=\; \sum_{y\in \mathcal{T}} \P{X = x, Y = y} 
 \]
 \vspace{4.5cm}
\end{frame}

\begin{frame} \frametitle{Conditional Probability}
  Random variables: $X,Y \colon \Omega \to \mathcal{T} $
  \[
    \P{ X = x \mid Y = y } \; =\;  \frac{\P{ X = x, Y = y }}{\P{ Y = y }}
  \] 
  \vspace{4.5cm}
\end{frame}

\begin{frame} \frametitle{Product Rule}
  Random variables: $X,Y \colon \Omega \to \mathcal{T} $
  \[
     \P{ X = x, Y = y } \; =\;  \P{ X = x \mid Y = y } \cdot \P{ Y = y }
  \] 
  \vspace{4.5cm}
\end{frame}

\begin{frame} \frametitle{Independence}
%   \textbf{Random events} $A, B \subset \mathcal{F}$
% \begin{align*}
%   P(A \cap B) &= P(A) \cdot P(B) \\
%   P(A \mid B) &= P(A)
% \end{align*} 
% \vfill 
Random variables $X,Y \colon \Omega \to \Real$ are \textbf{independent} if for all $x,y \in \Real$:
\[\P{ X = x,Y = y } = \P{ X = x } \cdot  \P{ Y = y } \]
Examples?\\
\vspace{4cm}
\end{frame}

\begin{frame} \frametitle{Bayes Theorem}
  Random variables: $X,Y \colon \Omega \to \mathcal{T} $
  \[
   \P{ X = x \mid Y = y } =
    \frac{\P{ Y = y \mid X = x } \cdot \P{ X = x }}{\P{ Y = y }}
  \]
  Proof:
  \vspace{4cm}
\end{frame}

\begin{frame} \frametitle{Inference Problem}
  \[
    \P{ X = x \mid Y = y } =
    \frac{\P{ Y = y \mid X = x } \cdot \P{ X = x }}{\P{ Y = y }}
  \]
  Prevalence: 1\%, Pos. when sick: 90\%, Neg. when healthy: 95\%\\
  Probability of sick when positive test:
  \vspace{4cm}
\end{frame}

\begin{frame} \frametitle{Discrete Probability Distributions}
    \begin{itemize}
    \item \textbf{Bernoulli}: Heads or tails
    \item \textbf{Binomial}: Number of heads
    \item \textbf{Geometric}: Coin flips until heads
    \item \textbf{Poisson}: Number of customers 
    \end{itemize}
  {\small See Wikipedia for their properties}
\end{frame}

\begin{frame} \frametitle{Continuous Random Variable}
  Usually real-valued: $X\colon \Omega \to \Real$ \\
  \vfill 
  \textbf{Example}: Probability space
  \begin{itemize}
  \item $\Omega = [0,1]$
  \item $\mathcal{F}$ = Borel $\sigma$-algebra of all intervals (open and closed)
  \item $P([a,b]) = b-a$ for $a \le  b$ (Lebesgue mesure)
  \end{itemize}
  \vfill 
  Random variable: $X(\omega) = \omega$ for each $\omega\in \Omega$ \\
  \vfill
  \[
   \P{X = 0.5} = \qquad  \qquad \qquad ? 
  \]
\end{frame}

\begin{frame} \frametitle{Cumulative Distribution Function}
  \[
    F_X(x) \; =\;  \P{X \le x} = 
    P \left( \left\{ \omega \in  \Omega \mid X(\omega) \le x \right\} \right)
  \]
  Example: Uniform random variable on $[0,1]$
  \vspace{5cm}
\end{frame}

\begin{frame} \frametitle{Probability Density Function}
  Function $f_X\colon \Real \to \Real$ is a pdf of $X$ if $\forall x$
  \[
   F_X(x)  = \int_{-\infty}^x f_X(t) dt 
 \]
 \vspace{4cm}
\end{frame}

\begin{frame} \frametitle{Atomic Random Variables}
  Random variable $X$ need not have pdf! \\
  Let $\P{X = 0} = \P{X = 1} = \P{X = 2} = \frac{1}{3}$\\
  \vspace{5cm}
\end{frame}

\begin{frame}\frametitle{Continuous Probability Distributions} 
    \begin{itemize}
    \item \textbf{Normal}: common because of central limit theorem
    \item \textbf{Laplace}: Extreme weather events
    \item \textbf{Multivariate normal}: Height and weight
    \end{itemize}
  {\small See Wikipedia for their properties}
\end{frame}


\begin{frame} \frametitle{Conditional Probability Density Function}
  For random variables $X, Y$ with a joint density $f_{X,Y}\colon \Real^2 \to \Real$, the conditional density is (when $f_X(x) > 0$):
  \[
   f_{Y \mid X}(y \mid  x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
 \]
 \vspace{4cm}
\end{frame}

% \begin{frame} \frametitle{Independence}
% %   \textbf{Random events} $A, B \subset \mathcal{F}$
% % \begin{align*}
% %   P(A \cap B) &= P(A) \cdot P(B) \\
% %   P(A \mid B) &= P(A)
% % \end{align*} 
% % \vfill 
% Random variable $X,Y \colon \Omega \to \Real$ are \textbf{independent} if for all $x,y \in \Real$:
% \begin{align*}
%   \P{ X \le x, Y \le y } &= \P{ X \le x } \cdot  \P{ Y \le y } \\
%   \P{ X \le x \mid Y \le y } &= \P{ X \le  x }
% \end{align*}
% \end{frame}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
